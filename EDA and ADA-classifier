# -*- coding: utf-8 -*-
"""Adaboost heart disease dataset.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/15K3R45QK5A5ouGvqJWaRlkkrGWuCRz60
"""

from google.colab import drive
drive.mount('/content/drive')

import pandas as pd
import numpy as np
import sklearn as sk
import seaborn as sns
import matplotlib.pyplot as plt

df=pd.read_csv('/content/drive/MyDrive/Colab Notebooks/Heart disease dataset/heart.csv')

df.rename(columns={'trestbps': 'resting_blood_pressure', 'chol': 'cholesterol', 'fbs': 'fasting_blood_sugar',
                   'restecg': 'resting_electrocardiographic_results', 'thalach': 'max_heartrate', 
                   'exang': 'exercise_induced_angina', 'slope': 'slope_peak_exercise', 'ca': 'ca_major_bloodvessels_light', 'thal': 'blood_disord_test', 'cp': 'chestpain_type', 'oldpeak' :'st_segment_depression' }, inplace=True)

df.head(25)

df.describe().round(2)

##Data cleaning##
# missing values visualiseren met een heatmap
plt.figure(figsize=(15,6))
cols = df.columns
sns.heatmap(df[cols].isnull(), cmap="YlGnBu",
            cbar_kws={'label': 'Missende data'})

#missing values per feature - https://pythoncursus.nl/data-cleaning-python/
for column in df.columns:
  # print(column)
  number_missing=np.sum(df[column].isnull())
  print('{} - {}'.format(column, number_missing))

#https://12ft.io/proxy?q=https%3A%2F%2Ftowardsdatascience.com%2Fheatmap-basics-with-pythons-seaborn-fb92ea280a6c
df_corr= df.corr().round(2)

# mask
mask = np.triu(np.ones_like(df_corr, dtype=np.bool_))
# adjust mask and df
mask = mask[1:, :-1]
corr = df_corr.iloc[1:,:-1].copy()

fig, ax = plt.subplots(figsize=(12, 10))
# mask
mask = np.triu(np.ones_like(df_corr, dtype=np.bool_))

# color map
cmap = sns.diverging_palette(240, 10, n=9, as_cmap=True)
# plot heatmap
sns.heatmap(df_corr, mask=mask, annot=True, fmt=".2f", 
           linewidths=5, cmap=cmap, vmin=-1, vmax=1, 
           cbar_kws={"shrink": .8}, square=True)
# ticks
yticks = [i.upper() for i in df_corr.index]
xticks = [i.upper() for i in df_corr.columns]
plt.yticks(plt.yticks()[0], labels=yticks, rotation=0)
plt.xticks(plt.xticks()[0], labels=xticks)
# title
title = 'Correlation Matrix for heart disease indicators'
plt.title(title, loc='left', fontsize=18)
plt.show()

#Max heart rate and resting blood pressure have medium correlation (0,30)
# no clear relationship, also not when sex is added.
sns.scatterplot(data=df, x="age", y="max_heartrate", hue="sex")

#age and heartrate are highly correlated (-0,40)
#on average, with young people, a higher maximum heartrate indicates they have heartdisease
sns.lmplot(x="age", y="max_heartrate", hue='target', data=df,
           order=2, ci=None, scatter_kws={"s": 80});

#ST depression induced by exercise relative to rest (0-7) is highly correlated (-0,58) with the slope of the peak exercise ST segment (1-3)
#People with low slope of the peak exercise have heart disease
sns.scatterplot(data=df, x="slope_peak_exercise", y="st_segment_depression", hue="target")

#low st segment depression is more people with heart disease
sns.lmplot(x="st_segment_depression", y="target", data=df, 
           logistic=True, ci=None, scatter_kws={"s": 80});

from sklearn.model_selection import cross_val_score
from sklearn.tree import DecisionTreeClassifier
from sklearn.ensemble import AdaBoostClassifier
from numpy import mean
from numpy import std

array=df.values
X=array[:,1:13]
y= array[:,13]
print(X.shape)
print(y.shape)

# #Split data, but we used K-fold, which is why we do not use this one. Unless we want to test a single accuracy output
from pandas.core.common import random_state
from sklearn.model_selection import train_test_split
# Separate out a validation dataset.
X_train, X_test, y_train, y_test = train_test_split(X, y,test_size=0.35, random_state=0)
print(df.shape)
print (X_train.shape)
print (y_train.shape)
print (X_test.shape)
print (y_test.shape)

#to test a single accuracy output
n_estimators=150
learning_rate=0.05
dt_stump = DecisionTreeClassifier(max_depth=1, min_samples_leaf=1)

ada_real= AdaBoostClassifier(
    base_estimator= dt_stump,
    learning_rate=learning_rate,
    n_estimators=n_estimators,
    algorithm="SAMME.R")

ada_real.fit(X_train, y_train)
ada_real.predict(X_test)
ada_real.score(X_test, y_test)


#calculate the average score with different hyperparameters (learning rate and N_Estimators)
dt_stump = DecisionTreeClassifier(max_depth=1, min_samples_leaf=1)
for LR in np.arange(0.05, 0.55, 0.1 ):
  for EST in range(50,350,100):

    ada_real= AdaBoostClassifier(
      base_estimator= dt_stump,
      learning_rate=LR,
      n_estimators=EST,
      algorithm="SAMME.R")
    
    dt_stump = DecisionTreeClassifier(max_depth=1, min_samples_leaf=1)
    n_scores = cross_val_score(ada_real, X, y, scoring='accuracy', cv=rskf, error_score='raise')

    print('Learning rate: %.2f ~ N_estimators: %.0f ~ Accuracy: %.3f (%.3f)' % (LR, EST, mean(n_scores), std(n_scores)))


#Confusion matrix AOC etc.
from sklearn.metrics import confusion_matrix
from sklearn.metrics import ConfusionMatrixDisplay
from sklearn.model_selection import RepeatedStratifiedKFold
conf_matrix_list_of_arrays = []

n_estimators=150
learning_rate=0.05
dt_stump = DecisionTreeClassifier(max_depth=1, min_samples_leaf=1)

ada_real= AdaBoostClassifier(
    base_estimator= dt_stump,
    learning_rate=learning_rate,
    n_estimators=n_estimators,
    algorithm="SAMME.R")

rskf= RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=1)
for train_index, test_index in rskf.split(X, y):
  # print("TRAIN:", train_index, "TEST:", test_index)
  X_train, X_test = X[train_index], X[test_index]
  y_train, y_test = y[train_index], y[test_index]


  ada_real.fit(X_train, y_train)
  conf_matrix=confusion_matrix(y_test, ada_real.predict(X_test))
  conf_matrix_list_of_arrays.append(conf_matrix)

  # print(conf_matrix_list_of_arrays)

mean_of_conf_matrix_arrays = np.mean(conf_matrix_list_of_arrays, axis=0)

print(mean_of_conf_matrix_arrays.round(1))

#plot the average confusion matrix
import seaborn as sns
sns.heatmap(mean_of_conf_matrix_arrays, annot=True, cmap='Blues')

!pip install eli5

# mean_of_conf_matrix_arrays=np.array(mean_of_conf_matrix_arrays)
def myscores(mean_of_conf_matrix_arrays): 
    tp = mean_of_conf_matrix_arrays[0][0] 
    fp = mean_of_conf_matrix_arrays[0][1] 
    fn = mean_of_conf_matrix_arrays[1][0] 
    tn = mean_of_conf_matrix_arrays[1][1] 
    return tp/(tp+fp), tp/(tp+fn), tp/(tp+fn)

print("precision, recall/sensitivity, specificity:", myscores(mean_of_conf_matrix_arrays))


import eli5
from sklearn.inspection import permutation_importance

clf = ada_real.fit(X_test, y_test)
result = permutation_importance(clf, X_test, y_test, n_repeats=10, random_state=0)

result.importances_mean
